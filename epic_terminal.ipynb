{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m╔══════════════════════════════════════════════════════════════════════════════╗\u001b[0m\n",
      "\u001b[35m║\u001b[0m                   Token Linear Architecture Visualization                    \u001b[35m║\u001b[0m\n",
      "\u001b[35m╚══════════════════════════════════════════════════════════════════════════════╝\u001b[0m\n",
      "\n",
      "\u001b[36m=== Token Activation Patterns ===\u001b[0m\n",
      "Token  0: [🔴🔴🔴                                               ] 0.061\n",
      "Token  1: [🔴🔴🔴                                               ] 0.064\n",
      "Token  2: [🔴🔴🔴                                               ] 0.066\n",
      "Token  3: [🔴🔴                                                ] 0.058\n",
      "Token  4: [🔴🔴                                                ] 0.053\n",
      "Token  5: [🔴🔴                                                ] 0.058\n",
      "Token  6: [🔴🔴🔴                                               ] 0.079\n",
      "Token  7: [🔴🔴🔴                                               ] 0.061\n",
      "Token  8: [🔴🔴                                                ] 0.047\n",
      "Token  9: [🔴🔴🔴                                               ] 0.065\n",
      "Token 10: [🔴🔴🔴                                               ] 0.073\n",
      "Token 11: [🔴🔴🔴                                               ] 0.073\n",
      "Token 12: [🔴🔴                                                ] 0.054\n",
      "Token 13: [🔴🔴🔴                                               ] 0.070\n",
      "Token 14: [🔴🔴                                                ] 0.057\n",
      "Token 15: [🔴🔴🔴                                               ] 0.063\n",
      "\n",
      "\u001b[33m=== Attention Flow Animation ===\u001b[0m\n",
      "█████▓▓▓▓▓▓▓▓▓█████████▓▓▓▓▓▓▓████████████\n",
      "\n",
      "\u001b[32m=== Frequency Component Mixing ===\u001b[0m\n",
      "□∘∘∘∙∙∙∎∎∎∎∎∎∎∎∎∎∎∙∙∙∘∘□□□□□□□□□□□\n",
      "\n",
      "\u001b[35m╔══════════════════════════════════════════════════════════════════════════════╗\u001b[0m\n",
      "\u001b[35m║\u001b[0m                            Press Enter to exit...                            \u001b[35m║\u001b[0m\n",
      "\u001b[35m╚══════════════════════════════════════════════════════════════════════════════╝\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from typing import Tuple, List\n",
    "import time\n",
    "import math\n",
    "from scipy.special import sph_harm\n",
    "\n",
    "class TokenLinearVisualizer:\n",
    "    def __init__(self, in_features: int, out_features: int, num_tokens: int):\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.num_tokens = num_tokens\n",
    "        \n",
    "        # Enhanced character sets for different aspects\n",
    "        self.token_chars = \"🔴🟡🟢🔵🟣⚪️⚫️🟤\"  # Token activation\n",
    "        self.attention_chars = \"█▓▒░ \"  # Attention weights\n",
    "        self.frequency_chars = \"∎∙∘□ \"  # Frequency components\n",
    "        \n",
    "    def visualize_token_activation(self, tokens: torch.Tensor, width: int = 50):\n",
    "        \"\"\"Visualize token activation patterns\"\"\"\n",
    "        # Normalize token activations\n",
    "        normalized = F.softmax(tokens.mean(dim=-1), dim=-1)\n",
    "        \n",
    "        print(\"\\n\\033[36m=== Token Activation Patterns ===\\033[0m\")\n",
    "        for i, activation in enumerate(normalized):\n",
    "            level = int(activation.item() * (len(self.token_chars) - 1))\n",
    "            bar = self.token_chars[level] * int(activation.item() * width)\n",
    "            print(f\"Token {i:2d}: [{bar:<{width}}] {activation.item():.3f}\")\n",
    "            \n",
    "    def visualize_attention_flow(self, q: torch.Tensor, k: torch.Tensor, frame_count: int = 30):\n",
    "        \"\"\"Animate attention flow between query and key spaces\"\"\"\n",
    "        print(\"\\n\\033[33m=== Attention Flow Animation ===\\033[0m\")\n",
    "        \n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(k.size(-1))\n",
    "        attention = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        for frame in range(frame_count):\n",
    "            # Create animated flow pattern\n",
    "            flow = \"\"\n",
    "            for i in range(attention.size(0)):\n",
    "                weight = attention[i].max().item()\n",
    "                idx = int(weight * (len(self.attention_chars) - 1))\n",
    "                char = self.attention_chars[idx]\n",
    "                \n",
    "                # Add pulsing effect based on frame\n",
    "                pulse = abs(math.sin(frame * 0.2 + i * 0.5))\n",
    "                width = int(weight * 20 * pulse) + 1\n",
    "                flow += char * width\n",
    "                \n",
    "            print(f\"\\r{flow}\", end=\"\", flush=True)\n",
    "            time.sleep(0.1)\n",
    "        print()\n",
    "        \n",
    "    def visualize_frequency_mixing(self, freq_tokens: torch.Tensor):\n",
    "        \"\"\"Visualize frequency components in token space\"\"\"\n",
    "        print(\"\\n\\033[32m=== Frequency Component Mixing ===\\033[0m\")\n",
    "        \n",
    "        # Generate harmonic basis\n",
    "        t = np.linspace(0, 2*np.pi, 100)\n",
    "        harmonics = []\n",
    "        \n",
    "        for l in range(2):  # Use first few spherical harmonics\n",
    "            for m in range(-l, l+1):\n",
    "                if l == 0 and m == 0:\n",
    "                    # Y00 term\n",
    "                    harmonics.append(np.ones_like(t) / np.sqrt(4 * np.pi))\n",
    "                elif l == 1:\n",
    "                    # Y1m terms\n",
    "                    if m == 0:\n",
    "                        harmonics.append(np.sqrt(3/(4*np.pi)) * np.cos(t))\n",
    "                    elif m == 1:\n",
    "                        harmonics.append(-np.sqrt(3/(8*np.pi)) * np.sin(t))\n",
    "                    else:  # m == -1\n",
    "                        harmonics.append(np.sqrt(3/(8*np.pi)) * np.sin(t))\n",
    "        \n",
    "        # Visualize harmonic mixing\n",
    "        harmonics = np.array(harmonics)\n",
    "        for frame in range(30):\n",
    "            mixed = np.zeros_like(t)\n",
    "            for i, harmonic in enumerate(harmonics):\n",
    "                mixed += harmonic * np.sin(frame * 0.2 + i * 0.5)\n",
    "            \n",
    "            # Normalize and visualize\n",
    "            mixed = (mixed - mixed.min()) / (mixed.max() - mixed.min())\n",
    "            \n",
    "            viz_line = \"\"\n",
    "            for val in mixed[::3]:  # Sample every 3rd point for display\n",
    "                idx = int(val * (len(self.frequency_chars) - 1))\n",
    "                viz_line += self.frequency_chars[idx]\n",
    "                \n",
    "            print(f\"\\r{viz_line}\", end=\"\", flush=True)\n",
    "            time.sleep(0.1)\n",
    "        print()\n",
    "\n",
    "def demo_visualization():\n",
    "    # Setup parameters\n",
    "    in_features = 64\n",
    "    out_features = 128\n",
    "    num_tokens = 16\n",
    "    \n",
    "    visualizer = TokenLinearVisualizer(in_features, out_features, num_tokens)\n",
    "    \n",
    "    # Generate sample data\n",
    "    tokens = torch.randn(num_tokens, in_features)\n",
    "    q = torch.randn(10, in_features)\n",
    "    k = torch.randn(num_tokens, in_features)\n",
    "    freq_tokens = torch.randn(num_tokens, in_features)\n",
    "    \n",
    "    # Run visualizations\n",
    "    print(\"\\033[35m╔\" + \"═\" * 78 + \"╗\\033[0m\")\n",
    "    print(\"\\033[35m║\\033[0m\" + f\"{'Token Linear Architecture Visualization':^78}\" + \"\\033[35m║\\033[0m\")\n",
    "    print(\"\\033[35m╚\" + \"═\" * 78 + \"╝\\033[0m\")\n",
    "    \n",
    "    visualizer.visualize_token_activation(tokens)\n",
    "    visualizer.visualize_attention_flow(q, k)\n",
    "    visualizer.visualize_frequency_mixing(freq_tokens)\n",
    "    \n",
    "    print(\"\\n\\033[35m╔\" + \"═\" * 78 + \"╗\\033[0m\")\n",
    "    print(\"\\033[35m║\\033[0m\" + f\"{'Press Enter to exit...':^78}\" + \"\\033[35m║\\033[0m\")\n",
    "    print(\"\\033[35m╚\" + \"═\" * 78 + \"╝\\033[0m\")\n",
    "    \n",
    "    input()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo_visualization()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
